
""" 
Author: Guilherme Marino Toso
Title: scpl
Project: Semi-Supervised Learning Using Competition for Neurons' Synchronization
Package: nsc

Description:
    This module contain the SCPL class wich implements the search method of the NSC
    algorithm as a classifier.

"""
import numpy as np
import pandas as pd
from nsc import Preprocessing
from .similarities import cosine, euclidean, hamming, imed, jaccard, manhattan
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import confusion_matrix
from scipy import sparse
from scipy.spatial.distance import cdist
import sys


"""
Classe Similaridade 
    função Inicializador
        Atributos:
            Escolhido
"""
class Similarity():
    def __init__(self):
        super().__init__()
        self.chosen = {
            'Euclidean': euclidean,
            'Hamming': hamming,
            'Manhattan':manhattan,
            'Jaccard':jaccard,
            'Cosine':cosine,
            'IMED': imed
        }


class SCPL():

    """ Simplicial Complex Propagating Labels (SCPL)

    .. py::class:

    This class uses the Simplicial Complex Propagating Labels (SCPL) Algorithm to classify data.
    The algorithm propagates the initial labels through an hypersphere, and for those data points whose
    closeness is inside the hypersphere, a 1-dimensional simplice is created between the data point, and the data point
    that generated the hypersphere, and then the label is propagated through this connection.

    .. py:function:: __init__(*args, **kwargs):

    :param similarity: The instantiation of the Similarity class.
    :type similarity: Similarity
    :param preprocessing: The instantiation of the Preprocessing class
    :type preprocessing: Preprocessing
    :param graph: The instantiation of the Graph class.
    :type graph: Graph
    :param accuracy: The accuracy percent array of every label classification.
    :type accuracy: numpy.ndarray.
    :param categorical: The categorical data that is used as features for the label propagation. By default is None.
    :type categorical: pandas.core.frame.DataFrame
    :param classified: A dictionare where the pair key, value represents the label lth and the list of the classified samples' indexes that are labeled with the lth label.
    :type classified: dict.
    :param data: The dataset with numerical and categorical (both for features and labels) data types. By default is None
    :type data: pandas.core.frame.DataFrame
    :param i_track: A dictionare where the pair key, value represents the shuffled index, true index for indexes track.
    :type i_track: dict
    :param labeled: A dictionare where the pair key, value represents the label lth and the list of the classified samples that are labeled with the lth label.
    :type labeled: dict
    :param labels: The list of total labels
    :type labels: list
    :param labels_indexes: A dictionaire where the pair key, value represents the label lth (str) and the list of the initial classified samples'indexes that are labeled with the lth label.
    :type labels_indexes: dict
    :param numerical: The numerical data that is used as features for the label propagation. By default is None.
    :type numerical: pandas.core.frame.DataFrame
    :param range: A list that contains the amount of classified data per threshold value.
    :type range: list
    :param target: The column name where the labels will propagate.
    :type target: str
    :param threshold: The hypersphere radius generated by every classified data.
    :type threshold: float
    :param threshold_list: A list that contain every threshold value between a initial value and the one that classifies all the data.
    :type threshold_list: list
    :param unlabel_indexes: A list of the samples's indexes that has no label.
    :type unlabel_indexes: list
    :param unlabeled: An array that contain the unlabeled data.
    :type unlabeled: nupy.ndarray
    :param X: The data that is used as features for the label propagation. By default is None.
    :type X: pandas.core.frame.DataFrame
    :param Y: The data that is used for the propagation to another samples. By default is None.
    :type Y: pandas.core.frame.DataFrame
    :param y_predicted: An array of -1 of the samples size that represents the array of predictions.
    :type y_predicted: numpy.ndarray
    :param y_true: An array of true labels that will be used to compare withh y_predicted.
    :type y_true: numpy.ndarray



    """
    def __init__(self, **kwargs):
        
        super().__init__()

        """ Instantiations """
        self.similarity = Similarity()
#        self.distance = Distance()
 #       self.graph = Graph()
        self.preprocessing = Preprocessing()

        """ Keyword Arguments """
        self.accuracy = None
        self.categorical = kwargs.get('categorical')
        self.classified = None
        self.data = kwargs.get('data')
        self.distance_name = kwargs.get('similarity')
        self.i_track = kwargs.get("i_track")
        self.labeled = kwargs.get('labeled')
        self.labels = kwargs.get('labels')
        self.labels_indexes = kwargs.get('labels_indexes')
        self.numerical = kwargs.get('numerical')
        self.range = []
        self.target = kwargs.get('target')
        self.threshold = kwargs.get('threshold')
        self.threshold_list = []
        self.unlabel_indexes = kwargs.get('unlabel_indexes')
        self.unlabeled = kwargs.get('unlabeled')
        self.X = kwargs.get('X')
        self.Y = kwargs.get('Y')
        self.y_predicted = kwargs.get('y_predict')
        self.y_true = kwargs.get('y_true')
        
        if isinstance(self.distance_name,str):
            self.distance = self.similarity.chosen[self.distance_name]
        
        

    def data_process(self, int_labels = True, split=True, set_null = False, \
        label_size=None, label_dict=None, get_indexes = True, zscore=False, \
        pca_var=None, dropnan=False, fillnan=False, shuffle=False):

        """ 
        
        .. py::function::

        This function preprocess the data for the label propagation
        
        :param int_labels: Define if the labels will be integers (True) or keep the same (False). 
                          By default is True.
        :type int_labels: bool.
        :param split: Define if will split (True) or not (False) the data into features, categorical, 
                     numerical and output. Default is True
        :type split: bool.
        :param set_null: Define if the unlabeled target will be set to null (True) or not (False). Default is False.
        :type set_null: bool.
        :param label_size: The amount of data that will keep  randomly labeled, and the rest will be unlabeled. Default None.
        :type label_size: int.
        :param label_dict: A dict of {label: list of indexes} per label defined by the user. Default None
        :type label_dict: dict.
        :param get_indexes: Define if it will get the indexes of the unlabeled and labeled data. Defaul is True.
        :type get_indexes: bool.
        :param zscore: Define if it will calculate the zscore of the data. Default is False.
        :type zscore: bool.
        :param pca_var: A value between 0 and 1 that represents the cum variance. If it sets a value, the pca will be 
                        calculated and the data will be sliced at the attribute where it reachs the cum variance.
        :type pca_var: float.
        :param dropnan: Define if it will drop NaN values (True) or not (False). Default is False.
        :type dropnan: bool.
        :param fillnan: Define if it will fill in the NaN values (True) or not (False) using a normal distribution generated by the known values. Default is False.
        :type fillnan: bool.
        :param shuffle: Define if it will shuffle the data or not.
        :type shuffle: bool.
        """

        self.labels = self.preprocessing.get_labels(self.data, self.target)

        if isinstance(int_labels, bool) and int_labels==True:

            for i in range(len(self.labels)):

                self.data.loc[self.data[self.target] == self.labels[i], self.target] = i

            self.labels = [i for i in range(len(self.labels))]

        if self.data.isna().any(axis=None) == True:

            if isinstance(dropnan,bool) and dropnan == True and isinstance(fillnan,bool) and fillnan==False:
                
                self.data = self.data.dropna()
                self.data = self.data.reset_index(drop=True)

            elif isinstance(dropnan,bool) and dropnan == False and isinstance(fillnan,bool) and fillnan==True:

                numerical = self.data.select_dtypes(include=np.number)
                columns = numerical.columns

                for i in range(len(columns)):

                    mean = numerical.loc[numerical[columns[i]].notna()].mean()[i]
                    std = numerical.loc[numerical[columns[i]].notna()].std()[i]
                    size = numerical.loc[numerical[columns[i]].isnull()].shape[0]
                    numerical[columns[i]].loc[numerical[columns[i]].isnull()] = np.random.normal(mean, std, size=(size))
                
                self.data.loc[:,columns] = numerical
        
            elif isinstance(dropnan,bool) and dropnan == True and isinstance(fillnan,bool) and fillnan==True:
        
                raise Exception("Please, choose only one option, to Drop NaN values or Fill in. Both, it makes harder to choose. I'm not Skynet yet.")
        if isinstance(split, bool) and split==True:

            self.X, self.categorical, self.numerical, self.Y = self.preprocessing.split_data(self.data, self.target)
            self.y_predicted = - np.ones(shape=(self.data.shape[0]))
            self.y_true = self.Y.copy()
        
        
        if isinstance(zscore, bool) and zscore == True:

            scaler = StandardScaler()

            self.numerical = scaler.fit_transform(self.numerical)
        
        if isinstance(pca_var,float) and 0 <= pca_var <= 1:

            if zscore == True:
                pass
            else:
                scaler = StandardScaler()
                self.numerical = scaler.fit_transform(self.numerical)
            
            pca = PCA()
            self.numerical = pca.fit_transform(self.numerical)
            pca_variance = pca.explained_variance_ratio_
            var_cum = np.cumsum(pca_variance)
            if pca_var < 1:
                index = np.where((var_cum >= pca_var))[0][0]
                self.numerical = self.numerical[:,:index]
            elif pca_var == 1:
                pass

        
        if isinstance(shuffle,bool) and shuffle == True:

            self.data = self.data.sample(frac=1)
            self.indexes = self.data.index
            self.data = self.data.reset_index(drop=True)
            
        
        if isinstance(set_null, bool) and set_null==True:
            
            if isinstance(label_size, int) and isinstance(label_dict,type(None)):    
                self.data = self.preprocessing.set_null_values(self.data, self.target, self.labels, label_size=label_size)
            elif isinstance(label_dict, dict) and isinstance(label_size,type(None)):
                self.data = self.preprocessing.set_null_values(self.data, self.target, self.labels, label_dict=label_dict)
            elif isinstance(label_size, type(None)) and isinstance(label_dict,type(None)):
                array = self.data.copy().to_numpy()[:,-1]
                labels, counts = np.unique(array, return_counts=True)
                label_size = int(0.1*min(counts))
                self.data = self.preprocessing.set_null_values(self.data, self.target, self.labels, label_size=label_size)               
        


        if isinstance(get_indexes, bool) and get_indexes == True:

            self.unlabel_indexes, self.labels_indexes = self.preprocessing.get_label_unlabel_inds(self.data, self.target, self.labels)

        if isinstance(self.numerical, pd.core.frame.DataFrame):

            self.numerical = self.numerical.values
        
        self.range = []
        self.threshold_list = []
        self.classified = {self.labels[i]:np.array([]) for i in range(len(self.labels))}

    
    
    def chunks(self):

        """ 
        .. py::function::

        This function separates the data into chunks of unlabeled and labeled data.
        
        """

        if isinstance(self.numerical, pd.core.frame.DataFrame):

            self.numerical = self.numerical.to_numpy()
        
        self.unlabeled = self.numerical[self.unlabel_indexes,:]

        self.labeled = {}

        self.labels_list = []
        for label, indexes in self.labels_indexes.items():
            self.labeled[label] = self.numerical[indexes,:]
            self.labels_list += indexes
        self.labels_list.sort()
        self.labels_list.append(self.data.shape[0]+1)
        
        self.classified = {self.labels[i]:np.array([]) for i in range(len(self.labels))}
    
    def calculate_similarity(self, x, y, axis=1):
        return self.distance(x,y,axis=axis)
    
    def propagation(self, threshold, data_dtype):

        """ 
        
        .. py::function:: 
        
        This function propagates the labels creating a simplicial complex of 1-dimension.
        
        :param threshold: The hypersphere radius that every classified dta creates. None by default.
        :type threshold: integer or float.
         
        """
        
        self.threshold = threshold

        cache = np.array([])
        labels_cache = np.array([])

        There_Is_Unlalebed = True

        len_cache = [0,0]

        iteration = 0
        
        while There_Is_Unlalebed:

            len_cache[0], len_cache[1] = len_cache[1], len_cache[0]

            for i in range(len(self.labels)):

                distance = self.calculate_similarity(
                    data_dtype[self.unlabel_indexes,None,:],
                    data_dtype[self.labels_indexes[i],:],
                    axis=2
                )
                rows, cols = np.where(distance <= self.threshold)
                classified_inds = self.unlabel_indexes[np.unique(rows)]

                cache = np.hstack((cache, classified_inds)).astype(int)
                labels_cache =np.hstack((labels_cache, np.zeros(shape=classified_inds.size) + i)).astype(int)
                self.classified[self.labels[i]] = np.hstack((self.classified[self.labels[i]], classified_inds)).astype(int)

            values, count = np.unique(cache, return_counts=True)
            recidivist = values[count>1]

            if recidivist.size > 0:

                for i in recidivist:

                    reidivist_index = np.where(cache==i)

                    intersec_labels = labels_cache[cache==i]

                    minimums = []

                    for j in range(intersec_labels.size):

                        new_distance = self.calculate_similarity(
                            data_dtype[None,i,:],
                            data_dtype[self.labels_indexes[intersec_labels[j]],:],
                            axis=1
                        )
    
                        
                        minimums.append(new_distance.min())

                        self.classified[intersec_labels[j]] = np.setdiff1d(self.classified[intersec_labels[j]],i)

                    min_index = minimums.index(min(minimums))

                    self.classified[intersec_labels[min_index]] = np.hstack((self.classified[intersec_labels[min_index]], i)) 

                    wrong_indexes = np.delete(reidivist_index, min_index)

                    cache = np.delete(cache, wrong_indexes)
                    labels_cache = np.delete(labels_cache, wrong_indexes)

            len_cache[1] = cache.size
            #print("Iteration {} Processed ------------------------------------------".format(iteration))
            #print("{} from {} unlabeled data points were classified \n".format(cache.size, self.unlabel_indexes.size))
            
            if cache.size == self.unlabel_indexes.size:

                #print("\n All Data Points were classified! \n -------------------------- Program Finished --------------------------")
                There_Is_Unlalebed = False
            
            elif len_cache[0] == len_cache[1] and iteration > 0:

                #print("\n At Iteration {} and {}, {} data points were classified. \nThere are no data points to classify. \n \
                #-------------------------- Program Finished --------------------------".format(iteration, iteration -1, len_cache[1]))
                There_Is_Unlalebed = False

            iteration = iteration + 1
    
    def evaluate(self):

        """ 
        .. py::function::

        This function evaluates the accuracy of the label propagation
        in the Simplicial Complex, printing the accuracy in the screen.
        
        """

        self.cache = np.array([])
        labels, counts = np.unique(self.y_true, return_counts=True)
        for i in range(len(self.labels)):

            self.y_predicted[self.classified[self.labels[i]]] = self.labels[i]
            self.cache = np.hstack((self.cache, self.classified[self.labels[i]])).astype(int)
            counts[i] = counts[i] - len(self.labels_indexes[self.labels[i]])
        self.y_predicted = self.y_predicted[self.cache]
        self.y_true = self.y_true[self.cache]
        counts = np.array(counts)
        self.accuracy = np.diag(confusion_matrix(self.y_true, self.y_predicted)*100/counts)
        #print("Accuracy: ", self.accuracy)
        self.score = np.diag(confusion_matrix(self.y_true, self.y_predicted)).sum()*100/self.y_true.shape[0]
        
    
    def fit(self, data_dtype, epochs=100):

        
        """ 
        .. py::function::

        This function propagates the labels through the unlabeled data for every value
        of the hypersphere radius (threshold), varying from 6 times the Standard Deviation
        of the maximum attribute divided by the epochs (6*std.max()/epochs) until all epochs is
        completed (6*std.max()) or the threshold value enables that all the data are classified. 
        
        Why 6*std.max()? Because in a normal distribution 99% of the data falls into 3*std,
        so in a sphere with the data is in the center, the data will fall into both sides 2*(3*std). So to garantee
        that all data can be ranged, we will get the higher std of the attributes, that's why 6*std.max().

        :param data_dtype: the data training used for the algorithm
        :type param_dtype: numpy.ndarray
        :param epochs: Amount of iterations through the minimum (6*std.max()/epochs) and maximum (6*std.max()) thrsehold values.
        :type epochs: int
        
        """


        std = np.std(data_dtype, axis=0)
        threshold_limit = 6*std.max()
        step = threshold_limit/epochs
        
        value = step

        for i in range(epochs):

            self.propagation(value, data_dtype)

            classified = []
            initial = []
            for label, indexes in self.classified.items():
                classified += list(indexes)
                initial += list(self.labels_indexes[label])
            total = classified + initial

            if len(total) == self.data.shape[0]:
                
                self.threshold_list.append(value)
                self.range.append(len(classified))
                print("Finished at Epoch {}".format(i))
                break

                
            else:

                self.threshold_list.append(value)
                self.range.append(len(classified))
                value += step
                self.unlabel_indexes, label_indexes = self.preprocessing.get_label_unlabel_inds(self.data, self.target, self.labels)
                self.classified = {self.labels[i]:np.array([]) for i in range(len(self.labels))}
        self.evaluate()
        #print(" The Threshold limit is: ", self.threshold)
        self.y_predicted = -np.ones(shape=(self.data.shape[0]))
        self.y_true = self.Y.copy()
            